import os
import json
import asyncio
from typing import Dict, Any, Optional
from openai import AsyncOpenAI
from dataclasses import dataclass

@dataclass
class EvaluationResult:
    """LLMè¯„ä¼°ç»“æœ"""
    accuracy_score: float  # å‡†ç¡®æ€§è¯„åˆ† (0-10)
    completeness_score: float  # å®Œæ•´æ€§è¯„åˆ† (0-10)
    relevance_score: float  # ç›¸å…³æ€§è¯„åˆ† (0-10)
    clarity_score: float  # æ¸…æ™°åº¦è¯„åˆ† (0-10)
    overall_score: float  # æ€»ä½“è¯„åˆ† (0-10)
    reasoning: str  # è¯„ä¼°ç†ç”±
    strengths: str  # ä¼˜ç‚¹
    weaknesses: str  # ä¸è¶³
    suggestions: str  # æ”¹è¿›å»ºè®®

class LLMEvaluator:
    """ä½¿ç”¨å¤§æ¨¡å‹è¯„ä¼°æŸ¥è¯¢å›ç­”è´¨é‡çš„è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–LLMè¯„ä¼°å™¨"""
        self.config_path = config_path or os.path.join(os.path.dirname(__file__), "..", "..", "config", "model_config.json")
        self.client = None
        self.evaluation_config = None
        self._load_config()
        self._init_client()
    
    def _load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # è®¾ç½®è¯„ä¼°å™¨é…ç½®
            self.evaluation_config = {
                "provider": "openai",
                "model": "gpt-4o-mini",  # ä½¿ç”¨GPT-4o-miniè¿›è¡Œè¯„ä¼°
                "temperature": 0.1,
                "max_tokens": 2000
            }
            
            # å¦‚æœé…ç½®ä¸­æœ‰è¯„ä¼°å™¨è®¾ç½®ï¼Œä½¿ç”¨å®ƒ
            if "evaluator" in config.get("llm", {}).get("agents", {}):
                self.evaluation_config.update(config["llm"]["agents"]["evaluator"])
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            self.evaluation_config = {
                "provider": "openai",
                "model": "gpt-4o-mini",
                "temperature": 0.1,
                "max_tokens": 2000
            }
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        self.client = AsyncOpenAI(api_key=api_key)
    
    def _create_evaluation_prompt(self, query: str, response: str, query_type: str, expected_criteria: list) -> str:
        """åˆ›å»ºè¯„ä¼°æç¤ºè¯"""
        criteria_text = "ã€".join(expected_criteria) if expected_criteria else "æ— ç‰¹å®šæ ‡å‡†"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯é—®ç­”ç³»ç»Ÿè¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æŸ¥è¯¢å’Œå›ç­”è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

**ç”¨æˆ·æŸ¥è¯¢**: {query}
**æŸ¥è¯¢ç±»å‹**: {query_type}
**æœŸæœ›è¯„ä¼°æ ‡å‡†**: {criteria_text}

**ç³»ç»Ÿå›ç­”**: 
{response}

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦å¯¹å›ç­”è¿›è¡Œè¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦ç»™å‡º0-10åˆ†çš„è¯„åˆ†ï¼š

1. **å‡†ç¡®æ€§ (Accuracy)**: å›ç­”ä¸­çš„ä¿¡æ¯æ˜¯å¦å‡†ç¡®ã€äº‹å®æ˜¯å¦æ­£ç¡®
2. **å®Œæ•´æ€§ (Completeness)**: å›ç­”æ˜¯å¦æ¶µç›–äº†æŸ¥è¯¢çš„æ‰€æœ‰å…³é”®æ–¹é¢
3. **ç›¸å…³æ€§ (Relevance)**: å›ç­”æ˜¯å¦ç›´æ¥ç›¸å…³å¹¶æ»¡è¶³ç”¨æˆ·éœ€æ±‚
4. **æ¸…æ™°åº¦ (Clarity)**: å›ç­”æ˜¯å¦è¡¨è¾¾æ¸…æ™°ã€é€»è¾‘æ€§å¼ºã€æ˜“äºç†è§£
5. **æ€»ä½“è´¨é‡ (Overall)**: ç»¼åˆè€ƒè™‘ä»¥ä¸Šå› ç´ çš„æ•´ä½“è¯„ä»·

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š

{{
  "accuracy_score": æ•°å­—(0-10),
  "completeness_score": æ•°å­—(0-10),
  "relevance_score": æ•°å­—(0-10),
  "clarity_score": æ•°å­—(0-10),
  "overall_score": æ•°å­—(0-10),
  "reasoning": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±ï¼Œè¯´æ˜å„ä¸ªè¯„åˆ†çš„ä¾æ®",
  "strengths": "å›ç­”çš„ä¸»è¦ä¼˜ç‚¹",
  "weaknesses": "å›ç­”çš„ä¸»è¦ä¸è¶³",
  "suggestions": "å…·ä½“çš„æ”¹è¿›å»ºè®®"
}}

æ³¨æ„ï¼š
- è¯„åˆ†è¦å®¢è§‚å…¬æ­£ï¼ŒåŸºäºå­¦æœ¯æ ‡å‡†
- å¦‚æœå›ç­”åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œå‡†ç¡®æ€§è¯„åˆ†åº”æ˜¾è‘—é™ä½
- å¦‚æœå›ç­”æ²¡æœ‰ç›´æ¥å›åº”æŸ¥è¯¢ï¼Œç›¸å…³æ€§è¯„åˆ†åº”é™ä½
- å¦‚æœå›ç­”è¿‡äºç®€å•æˆ–è¿‡äºå¤æ‚ï¼Œç›¸åº”è°ƒæ•´å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¯„åˆ†
- æ€»ä½“è¯„åˆ†åº”ç»¼åˆåæ˜ å›ç­”çš„æ•´ä½“è´¨é‡"""

        return prompt
    
    async def evaluate_response(
        self, 
        query: str, 
        response: str, 
        query_type: str = "general",
        expected_criteria: list = None
    ) -> EvaluationResult:
        """ä½¿ç”¨LLMè¯„ä¼°æŸ¥è¯¢å›ç­”è´¨é‡"""
        try:
            # åˆ›å»ºè¯„ä¼°æç¤ºè¯
            prompt = self._create_evaluation_prompt(query, response, query_type, expected_criteria or [])
            
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            completion = await self.client.chat.completions.create(
                model=self.evaluation_config["model"],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯é—®ç­”ç³»ç»Ÿè¯„ä¼°ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.evaluation_config["temperature"],
                max_tokens=self.evaluation_config["max_tokens"]
            )
            
            # è§£æè¯„ä¼°ç»“æœ
            result_text = completion.choices[0].message.content.strip()
            
            # æå–JSONéƒ¨åˆ†
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                json_text = result_text[json_start:json_end].strip()
            elif "{" in result_text and "}" in result_text:
                json_start = result_text.find("{")
                json_end = result_text.rfind("}") + 1
                json_text = result_text[json_start:json_end]
            else:
                raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–JSONæ ¼å¼çš„è¯„ä¼°ç»“æœ")
            
            # è§£æJSON
            evaluation_data = json.loads(json_text)
            
            # åˆ›å»ºè¯„ä¼°ç»“æœå¯¹è±¡
            return EvaluationResult(
                accuracy_score=float(evaluation_data.get("accuracy_score", 0)),
                completeness_score=float(evaluation_data.get("completeness_score", 0)),
                relevance_score=float(evaluation_data.get("relevance_score", 0)),
                clarity_score=float(evaluation_data.get("clarity_score", 0)),
                overall_score=float(evaluation_data.get("overall_score", 0)),
                reasoning=evaluation_data.get("reasoning", ""),
                strengths=evaluation_data.get("strengths", ""),
                weaknesses=evaluation_data.get("weaknesses", ""),
                suggestions=evaluation_data.get("suggestions", "")
            )
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {result_text}")
            # è¿”å›é»˜è®¤ä½åˆ†è¯„ä¼°ç»“æœ
            return EvaluationResult(
                accuracy_score=3.0,
                completeness_score=3.0,
                relevance_score=3.0,
                clarity_score=3.0,
                overall_score=3.0,
                reasoning="è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°JSONè§£æé”™è¯¯",
                strengths="æ— æ³•è¯„ä¼°",
                weaknesses="è¯„ä¼°ç³»ç»Ÿé”™è¯¯",
                suggestions="è¯·æ£€æŸ¥è¯„ä¼°ç³»ç»Ÿé…ç½®"
            )
        
        except Exception as e:
            print(f"âš ï¸ LLMè¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ä½åˆ†è¯„ä¼°ç»“æœ
            return EvaluationResult(
                accuracy_score=2.0,
                completeness_score=2.0,
                relevance_score=2.0,
                clarity_score=2.0,
                overall_score=2.0,
                reasoning=f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                strengths="æ— æ³•è¯„ä¼°",
                weaknesses="è¯„ä¼°ç³»ç»Ÿæ•…éšœ",
                suggestions="è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®"
            )
    
    async def batch_evaluate(self, test_results: list) -> list:
        """æ‰¹é‡è¯„ä¼°å¤šä¸ªæµ‹è¯•ç»“æœ"""
        evaluated_results = []
        
        for i, result in enumerate(test_results):
            print(f"ğŸ¤– LLMè¯„ä¼°è¿›åº¦: {i+1}/{len(test_results)}")
            
            if hasattr(result, 'query') and hasattr(result, 'response') and result.response:
                # è¿›è¡ŒLLMè¯„ä¼°
                llm_eval = await self.evaluate_response(
                    query=result.query,
                    response=result.response,
                    query_type=getattr(result, 'category', 'general'),
                    expected_criteria=getattr(result, 'evaluation_criteria', [])
                )
                
                # å°†LLMè¯„ä¼°ç»“æœæ·»åŠ åˆ°æµ‹è¯•ç»“æœä¸­
                result.llm_evaluation = llm_eval
                
                # ç­‰å¾…ä¸€ä¸‹é¿å…APIé™åˆ¶
                await asyncio.sleep(0.5)
            
            evaluated_results.append(result)
        
        return evaluated_results

if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°å™¨
    async def test_evaluator():
        evaluator = LLMEvaluator()
        
        test_query = "Michael Porterçš„ç«äº‰æˆ˜ç•¥è®ºæ–‡å‘è¡¨åœ¨å“ªä¸€å¹´ï¼Ÿ"
        test_response = "Michael Porterçš„ç»å…¸è®ºæ–‡ã€Šç«äº‰æˆ˜ç•¥ã€‹(Competitive Strategy)å‘è¡¨äº1980å¹´ã€‚è¿™æœ¬ä¹¦å¥ å®šäº†ç°ä»£ç«äº‰åˆ†æçš„åŸºç¡€ï¼Œæå‡ºäº†è‘—åçš„äº”åŠ›æ¨¡å‹ã€‚"
        
        result = await evaluator.evaluate_response(
            query=test_query,
            response=test_response,
            query_type="basic_information",
            expected_criteria=["å‡†ç¡®çš„å¹´ä»½ä¿¡æ¯", "ç›¸å…³çš„è¡¥å……ä¿¡æ¯"]
        )
        
        print("ğŸ¯ è¯„ä¼°ç»“æœ:")
        print(f"å‡†ç¡®æ€§: {result.accuracy_score}/10")
        print(f"å®Œæ•´æ€§: {result.completeness_score}/10")
        print(f"ç›¸å…³æ€§: {result.relevance_score}/10")
        print(f"æ¸…æ™°åº¦: {result.clarity_score}/10")
        print(f"æ€»ä½“è¯„åˆ†: {result.overall_score}/10")
        print(f"è¯„ä¼°ç†ç”±: {result.reasoning}")
    
    asyncio.run(test_evaluator()) 