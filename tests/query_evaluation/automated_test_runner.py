#!/usr/bin/env python3
"""
CiteWeave Multi-Agent System Query Test Runner
è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨ - è¿è¡Œå„ç§ç±»å‹çš„å­¦æœ¯è®ºæ–‡æŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹
"""

import json
import asyncio
import sys
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import pandas as pd

# Add src to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from enhanced_multi_agent_system import EnhancedMultiAgentSystem
from graph_builder import GraphDB
from llm_evaluator import LLMEvaluator, EvaluationResult
from vector_indexer import VectorIndexer
from author_paper_index import AuthorPaperIndex
from config_manager import ConfigManager

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_id: str
    category: str
    query_cn: str
    query_en: str
    expected_result_type: str
    complexity: str
    evaluation_criteria: List[str]
    
    # è¿è¡Œç»“æœ
    success: bool = False
    response: str = ""
    confidence: float = 0.0
    query_type: str = ""
    action: str = ""
    response_language: str = ""
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # è¯„ä¼°ç»“æœ
    evaluation_scores: Dict[str, float] = field(default_factory=dict)
    overall_score: float = 0.0
    execution_time: float = 0.0
    notes: str = ""
    
    # LLMè¯„ä¼°ç»“æœ
    llm_evaluation: Optional[EvaluationResult] = None
    enhanced_score: float = 0.0  # åŒ…å«LLMè¯„ä¼°çš„å¢å¼ºè¯„åˆ†

class QueryTestRunner:
    """æŸ¥è¯¢æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_dir: str = "../../config", enable_llm_evaluation: bool = True):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.config_dir = config_dir
        self.agent_system = None
        self.test_cases = {}
        self.results = []
        self.enable_llm_evaluation = enable_llm_evaluation
        self.llm_evaluator = None
        
        # åˆå§‹åŒ–LLMè¯„ä¼°å™¨
        if self.enable_llm_evaluation:
            try:
                model_config_path = os.path.join(self.config_dir, "model_config.json")
                self.llm_evaluator = LLMEvaluator(model_config_path)
                print("âœ… LLMè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ LLMè¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯„ä¼°: {e}")
                self.enable_llm_evaluation = False
        
    async def initialize_system(self):
        """åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")
            
            # åŠ è½½é…ç½®
            config_manager = ConfigManager(self.config_dir)
            neo4j_config = config_manager.get_neo4j_config()
            
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            graph_db = GraphDB(
                uri=neo4j_config["uri"],
                user=neo4j_config["user"],
                password=neo4j_config["password"]
            )
            
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            
            vector_indexer = VectorIndexer(
                paper_root=os.path.join(project_root, "data", "papers"),
                index_path=os.path.join(project_root, "data", "vector_index")
            )
            
            # åˆå§‹åŒ–ä½œè€…ç´¢å¼•
            author_index = AuthorPaperIndex(
                storage_root=os.path.join(project_root, "data", "papers"),
                index_db_path=os.path.join(project_root, "data", "author_paper_index.db")
            )
            
            # åˆå§‹åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿ
            self.agent_system = EnhancedMultiAgentSystem(
                graph_db=graph_db,
                vector_indexer=vector_indexer,
                author_index=author_index,
                config_path=os.path.join(self.config_dir, "model_config.json")
            )
            
            print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def load_test_cases(self, test_file: str = "simplified_test_cases.json"):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            test_file_path = os.path.join(os.path.dirname(__file__), "..", "..", "test_data", test_file)
            with open(test_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.test_cases = data["test_cases_by_category"]
            
            print(f"ğŸ“ åŠ è½½äº† {len(self.test_cases)} ä¸ªæµ‹è¯•ç±»åˆ«")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
            return False
    
    async def run_single_test(self, category: str, test_case: Dict[str, Any], language: str = "cn") -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        start_time = datetime.now()
        
        # åˆ›å»ºæµ‹è¯•ç»“æœå¯¹è±¡
        result = TestResult(
            test_id=test_case["id"],
            category=category,
            query_cn=test_case["query_cn"],
            query_en=test_case["query_en"],
            expected_result_type=test_case["expected_result_type"],
            complexity=test_case["complexity"],
            evaluation_criteria=test_case["evaluation_criteria"]
        )
        
        try:
            # é€‰æ‹©æŸ¥è¯¢è¯­è¨€
            query = test_case["query_cn"] if language == "cn" else test_case["query_en"]
            
            # è¿è¡ŒæŸ¥è¯¢
            response = await self.agent_system.query(
                user_query=query,
                thread_id=f"test_{test_case['id']}",
                user_id="test_runner"
            )
            
            # è®°å½•ç»“æœ
            result.success = True
            result.response = response.get("response", "")
            result.confidence = response.get("confidence", 0.0)
            result.query_type = response.get("query_type", "")
            result.action = response.get("action", "")
            result.response_language = response.get("response_language", "")
            result.errors = response.get("errors", [])
            result.warnings = response.get("warnings", [])
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.now()
            result.execution_time = (end_time - start_time).total_seconds()
            
            # åŸºç¡€è¯„ä¼°
            result.overall_score = self._evaluate_response(result)
            
            # LLMè¯„ä¼° (å¦‚æœå¯ç”¨)
            if result.success and result.response:
                await self._perform_llm_evaluation(result)
                # ä½¿ç”¨å¢å¼ºè¯„åˆ†ä½œä¸ºæœ€ç»ˆè¯„åˆ†
                final_score = result.enhanced_score if result.enhanced_score > 0 else result.overall_score
                print(f"âœ… {test_case['id']}: åŸºç¡€è¯„åˆ†={result.overall_score:.1f}/10, æœ€ç»ˆè¯„åˆ†={final_score:.1f}/10")
            else:
                print(f"âœ… {test_case['id']}: {result.overall_score:.2f}/10.0")
            
        except Exception as e:
            result.success = False
            result.errors = [str(e)]
            result.overall_score = 0.0
            end_time = datetime.now()
            result.execution_time = (end_time - start_time).total_seconds()
            
            print(f"âŒ {test_case['id']}: {str(e)}")
        
        return result
    
    def _evaluate_response(self, result: TestResult) -> float:
        """ç®€å•çš„å“åº”è¯„ä¼°å‡½æ•°"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•° - æ˜¯å¦æˆåŠŸè¿”å›å“åº”
        if result.success and result.response:
            score += 3.0
        
        # ç½®ä¿¡åº¦åˆ†æ•°
        score += result.confidence * 2.0
        
        # å“åº”é•¿åº¦åˆç†æ€§ (100-2000å­—ç¬¦ä¸ºåˆç†èŒƒå›´)
        if 100 <= len(result.response) <= 2000:
            score += 2.0
        elif 50 <= len(result.response) < 100:
            score += 1.0
        
        # é”™è¯¯æƒ©ç½š
        if result.errors:
            score -= len(result.errors) * 0.5
        
        # è­¦å‘Šæƒ©ç½š
        if result.warnings:
            score -= len(result.warnings) * 0.2
        
        # å¤æ‚åº¦è°ƒæ•´
        if result.complexity == "high" and score >= 7.0:
            score += 1.0  # é«˜å¤æ‚åº¦é—®é¢˜çš„å¥–åŠ±
        
        return min(max(score, 0.0), 10.0)  # é™åˆ¶åœ¨0-10èŒƒå›´å†…
    
    async def _enhanced_evaluate_response(self, result: TestResult) -> float:
        """å¢å¼ºçš„å“åº”è¯„ä¼°å‡½æ•° - ç»“åˆåŸºç¡€è¯„ä¼°å’ŒLLMè¯„ä¼°"""
        # åŸºç¡€è¯„åˆ† (0-10åˆ†)
        basic_score = self._evaluate_response(result)
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨LLMè¯„ä¼°æˆ–LLMè¯„ä¼°å¤±è´¥ï¼Œè¿”å›åŸºç¡€è¯„åˆ†
        if not self.enable_llm_evaluation or not result.llm_evaluation:
            return basic_score
        
        # LLMè¯„ä¼°æƒé‡é…ç½®
        llm_weight = 0.7  # LLMè¯„ä¼°æƒé‡70%
        basic_weight = 0.3  # åŸºç¡€è¯„ä¼°æƒé‡30%
        
        # è®¡ç®—LLMç»¼åˆè¯„åˆ† (å–overall_score)
        llm_score = result.llm_evaluation.overall_score
        
        # è®¡ç®—åŠ æƒæœ€ç»ˆè¯„åˆ†
        enhanced_score = (llm_score * llm_weight) + (basic_score * basic_weight)
        
        return min(max(enhanced_score, 0.0), 10.0)  # é™åˆ¶åœ¨0-10èŒƒå›´å†…
    
    async def _perform_llm_evaluation(self, result: TestResult):
        """ä¸ºå•ä¸ªæµ‹è¯•ç»“æœæ‰§è¡ŒLLMè¯„ä¼°"""
        if not self.enable_llm_evaluation or not self.llm_evaluator:
            return
        
        try:
            # é€‰æ‹©æŸ¥è¯¢è¯­è¨€
            query = result.query_cn if result.response_language == "zh" else result.query_en
            
            # æ‰§è¡ŒLLMè¯„ä¼°
            llm_eval = await self.llm_evaluator.evaluate_response(
                query=query,
                response=result.response,
                query_type=result.category,
                expected_criteria=result.evaluation_criteria
            )
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            result.llm_evaluation = llm_eval
            
            # è®¡ç®—å¢å¼ºè¯„åˆ†
            result.enhanced_score = await self._enhanced_evaluate_response(result)
            
            print(f"ğŸ¤– LLMè¯„ä¼°å®Œæˆ - æ€»ä½“è¯„åˆ†: {llm_eval.overall_score:.1f}/10")
            
        except Exception as e:
            print(f"âš ï¸ LLMè¯„ä¼°å¤±è´¥: {e}")
            result.enhanced_score = result.overall_score
    
    async def run_category_tests(self, category: str, language: str = "cn") -> List[TestResult]:
        """è¿è¡Œç‰¹å®šç±»åˆ«çš„æ‰€æœ‰æµ‹è¯•"""
        if category not in self.test_cases:
            print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•ç±»åˆ«: {category}")
            return []
        
        category_info = self.test_cases[category]
        test_cases = category_info["test_cases"]
        
        print(f"\nğŸ” è¿è¡Œ '{category}' ç±»åˆ«æµ‹è¯• ({len(test_cases)} ä¸ªç”¨ä¾‹)")
        print(f"ğŸ“‹ æ£€ç´¢ç­–ç•¥: {category_info['retrieval_strategy']}")
        print("-" * 60)
        
        results = []
        for test_case in test_cases:
            result = await self.run_single_test(category, test_case, language)
            results.append(result)
            self.results.append(result)
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…APIé™åˆ¶
            await asyncio.sleep(1)
        
        return results
    
    async def run_all_tests(self, language: str = "cn") -> List[TestResult]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        print(f"\nğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ (è¯­è¨€: {language})")
        print("=" * 70)
        
        all_results = []
        for category in self.test_cases.keys():
            category_results = await self.run_category_tests(category, language)
            all_results.extend(category_results)
        
        return all_results
    
    def generate_report(self, output_file: str = None) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.results:
            return "æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä»¥ç”ŸæˆæŠ¥å‘Š"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        avg_score = sum(r.overall_score for r in self.results) / total_tests
        avg_time = sum(r.execution_time for r in self.results) / total_tests
        
        # LLMè¯„ä¼°ç»Ÿè®¡
        llm_evaluated_tests = sum(1 for r in self.results if r.llm_evaluation is not None)
        avg_enhanced_score = 0.0
        if llm_evaluated_tests > 0:
            enhanced_scores = [r.enhanced_score for r in self.results if r.enhanced_score > 0]
            avg_enhanced_score = sum(enhanced_scores) / len(enhanced_scores) if enhanced_scores else 0.0
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for result in self.results:
            if result.category not in category_stats:
                category_stats[result.category] = {
                    'total': 0, 'success': 0, 'scores': [], 'times': [], 
                    'llm_evaluated': 0, 'enhanced_scores': []
                }
            
            stats = category_stats[result.category]
            stats['total'] += 1
            if result.success:
                stats['success'] += 1
            stats['scores'].append(result.overall_score)
            stats['times'].append(result.execution_time)
            
            # LLMè¯„ä¼°ç»Ÿè®¡
            if result.llm_evaluation is not None:
                stats['llm_evaluated'] += 1
            if result.enhanced_score > 0:
                stats['enhanced_scores'].append(result.enhanced_score)
        
        # ç”ŸæˆæŠ¥å‘Š
        evaluation_mode = "å¢å¼ºè¯„ä¼° (åŸºç¡€è¯„ä¼° + LLMè¯„ä¼°)" if self.enable_llm_evaluation else "åŸºç¡€è¯„ä¼°"
        
        report = f"""
# CiteWeave å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
è¯„ä¼°æ¨¡å¼: {evaluation_mode}

## ğŸ“Š æµ‹è¯•æ€»è§ˆ
- **æ€»æµ‹è¯•æ•°**: {total_tests}
- **æˆåŠŸæ•°**: {successful_tests} ({successful_tests/total_tests*100:.1f}%)
- **åŸºç¡€å¹³å‡å¾—åˆ†**: {avg_score:.2f}/10.0
- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {avg_time:.2f}ç§’"""

        # æ·»åŠ LLMè¯„ä¼°ä¿¡æ¯
        if self.enable_llm_evaluation:
            report += f"""
- **LLMè¯„ä¼°è¦†ç›–**: {llm_evaluated_tests}/{total_tests} ({llm_evaluated_tests/total_tests*100:.1f}%)
- **å¢å¼ºå¹³å‡å¾—åˆ†**: {avg_enhanced_score:.2f}/10.0"""

        report += "\n\n## ğŸ“‹ åˆ†ç±»ç»Ÿè®¡\n"
        
        for category, stats in category_stats.items():
            avg_cat_score = sum(stats['scores']) / len(stats['scores'])
            avg_cat_time = sum(stats['times']) / len(stats['times'])
            success_rate = stats['success'] / stats['total'] * 100
            
            report += f"""
### {category}
- æµ‹è¯•æ•°: {stats['total']}
- æˆåŠŸç‡: {success_rate:.1f}%
- åŸºç¡€å¹³å‡å¾—åˆ†: {avg_cat_score:.2f}/10.0  
- å¹³å‡æ—¶é—´: {avg_cat_time:.2f}ç§’"""

            # æ·»åŠ LLMè¯„ä¼°ä¿¡æ¯
            if self.enable_llm_evaluation and stats['enhanced_scores']:
                avg_enhanced_cat_score = sum(stats['enhanced_scores']) / len(stats['enhanced_scores'])
                llm_coverage = stats['llm_evaluated'] / stats['total'] * 100
                report += f"""
- LLMè¯„ä¼°è¦†ç›–: {stats['llm_evaluated']}/{stats['total']} ({llm_coverage:.1f}%)
- å¢å¼ºå¹³å‡å¾—åˆ†: {avg_enhanced_cat_score:.2f}/10.0"""
            
            report += "\n"
        
        report += "\n## ğŸ” è¯¦ç»†ç»“æœ\n"
        
        for result in self.results:
            status = "âœ…" if result.success else "âŒ"
            report += f"""
### {result.test_id} - {result.category} {status}
- **æŸ¥è¯¢**: {result.query_cn}
- **åŸºç¡€å¾—åˆ†**: {result.overall_score:.2f}/10.0"""

            # æ·»åŠ å¢å¼ºè¯„åˆ†ä¿¡æ¯
            if result.enhanced_score > 0:
                report += f"""
- **å¢å¼ºå¾—åˆ†**: {result.enhanced_score:.2f}/10.0"""
            
            report += f"""
- **æ‰§è¡Œæ—¶é—´**: {result.execution_time:.2f}ç§’
- **ç½®ä¿¡åº¦**: {result.confidence:.2f}
"""
            
            # æ·»åŠ LLMè¯„ä¼°è¯¦æƒ…
            if result.llm_evaluation:
                llm_eval = result.llm_evaluation
                report += f"""
#### ğŸ¤– LLMè¯„ä¼°è¯¦æƒ…
- **å‡†ç¡®æ€§**: {llm_eval.accuracy_score:.1f}/10
- **å®Œæ•´æ€§**: {llm_eval.completeness_score:.1f}/10
- **ç›¸å…³æ€§**: {llm_eval.relevance_score:.1f}/10
- **æ¸…æ™°åº¦**: {llm_eval.clarity_score:.1f}/10
- **LLMæ€»ä½“è¯„åˆ†**: {llm_eval.overall_score:.1f}/10
- **è¯„ä¼°ç†ç”±**: {llm_eval.reasoning}
- **ä¼˜ç‚¹**: {llm_eval.strengths}
- **ä¸è¶³**: {llm_eval.weaknesses}
- **æ”¹è¿›å»ºè®®**: {llm_eval.suggestions}
"""
            
            if result.errors:
                report += f"- **é”™è¯¯**: {'; '.join(result.errors)}\n"
            if result.warnings:
                report += f"- **è­¦å‘Š**: {'; '.join(result.warnings)}\n"
        
        # ä¿å­˜æŠ¥å‘Š
        if output_file:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return report
    
    def export_results_csv(self, output_file: str):
        """å¯¼å‡ºç»“æœä¸ºCSVæ ¼å¼"""
        if not self.results:
            print("æ²¡æœ‰ç»“æœå¯ä»¥å¯¼å‡º")
            return
        
        # å‡†å¤‡æ•°æ®
        data = []
        for result in self.results:
            row = {
                'test_id': result.test_id,
                'category': result.category,
                'query_cn': result.query_cn,
                'query_en': result.query_en,
                'complexity': result.complexity,
                'success': result.success,
                'overall_score': result.overall_score,
                'confidence': result.confidence,
                'execution_time': result.execution_time,
                'response_length': len(result.response),
                'error_count': len(result.errors),
                'warning_count': len(result.warnings),
                'query_type': result.query_type,
                'action': result.action
            }
            
            # æ·»åŠ LLMè¯„ä¼°å­—æ®µ
            if result.llm_evaluation:
                row.update({
                    'llm_evaluated': True,
                    'enhanced_score': result.enhanced_score,
                    'llm_accuracy_score': result.llm_evaluation.accuracy_score,
                    'llm_completeness_score': result.llm_evaluation.completeness_score,
                    'llm_relevance_score': result.llm_evaluation.relevance_score,
                    'llm_clarity_score': result.llm_evaluation.clarity_score,
                    'llm_overall_score': result.llm_evaluation.overall_score,
                    'llm_reasoning': result.llm_evaluation.reasoning,
                    'llm_strengths': result.llm_evaluation.strengths,
                    'llm_weaknesses': result.llm_evaluation.weaknesses,
                    'llm_suggestions': result.llm_evaluation.suggestions
                })
            else:
                row.update({
                    'llm_evaluated': False,
                    'enhanced_score': result.overall_score,
                    'llm_accuracy_score': None,
                    'llm_completeness_score': None,
                    'llm_relevance_score': None,
                    'llm_clarity_score': None,
                    'llm_overall_score': None,
                    'llm_reasoning': None,
                    'llm_strengths': None,
                    'llm_weaknesses': None,
                    'llm_suggestions': None
                })
            
            data.append(row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(data)
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"ğŸ“Š ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ CiteWeave å¤šæ™ºèƒ½ä½“æŸ¥è¯¢æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 50)
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨LLMè¯„ä¼°
    print("\nğŸ¤– è¯„ä¼°æ¨¡å¼é€‰æ‹©:")
    print("1. åŸºç¡€è¯„ä¼° (å¿«é€Ÿï¼Œä»…åŸºäºé•¿åº¦ã€ç½®ä¿¡åº¦ç­‰)")
    print("2. å¢å¼ºè¯„ä¼° (ä½¿ç”¨GPT-4o-miniè¿›è¡Œå†…å®¹è´¨é‡è¯„ä¼°)")
    
    eval_choice = input("è¯·é€‰æ‹©è¯„ä¼°æ¨¡å¼ (1-2): ").strip()
    enable_llm_evaluation = eval_choice == "2"
    
    evaluation_mode = "å¢å¼ºè¯„ä¼°æ¨¡å¼" if enable_llm_evaluation else "åŸºç¡€è¯„ä¼°æ¨¡å¼"
    print(f"âœ… å·²é€‰æ‹©: {evaluation_mode}")
    
    # åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
    runner = QueryTestRunner(enable_llm_evaluation=enable_llm_evaluation)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    if not await runner.initialize_system():
        print("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    if not runner.load_test_cases():
        print("æµ‹è¯•ç”¨ä¾‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # äº¤äº’å¼é€‰æ‹©
    print("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. è¿è¡Œæ‰€æœ‰æµ‹è¯• (ä¸­æ–‡)")
    print("2. è¿è¡Œæ‰€æœ‰æµ‹è¯• (è‹±æ–‡)")
    print("3. è¿è¡Œç‰¹å®šç±»åˆ«æµ‹è¯•")
    print("4. æ˜¾ç¤ºæµ‹è¯•ç±»åˆ«åˆ—è¡¨")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if choice == "1":
        # è¿è¡Œæ‰€æœ‰æµ‹è¯• (ä¸­æ–‡)
        await runner.run_all_tests("cn")
        report = runner.generate_report(f"test_reports/full_test_report_cn_{timestamp}.md")
        runner.export_results_csv(f"test_reports/full_test_results_cn_{timestamp}.csv")
        
    elif choice == "2":
        # è¿è¡Œæ‰€æœ‰æµ‹è¯• (è‹±æ–‡)
        await runner.run_all_tests("en")
        report = runner.generate_report(f"test_reports/full_test_report_en_{timestamp}.md")
        runner.export_results_csv(f"test_reports/full_test_results_en_{timestamp}.csv")
        
    elif choice == "3":
        # è¿è¡Œç‰¹å®šç±»åˆ«
        print("\nå¯ç”¨çš„æµ‹è¯•ç±»åˆ«:")
        for i, category in enumerate(runner.test_cases.keys(), 1):
            print(f"{i}. {category}")
        
        cat_choice = input("è¯·é€‰æ‹©ç±»åˆ«ç¼–å·: ").strip()
        try:
            category_list = list(runner.test_cases.keys())
            selected_category = category_list[int(cat_choice) - 1]
            
            language = input("é€‰æ‹©è¯­è¨€ (cn/en): ").strip() or "cn"
            
            await runner.run_category_tests(selected_category, language)
            report = runner.generate_report(f"test_reports/{selected_category}_test_report_{timestamp}.md")
            runner.export_results_csv(f"test_reports/{selected_category}_test_results_{timestamp}.csv")
            
        except (ValueError, IndexError):
            print("æ— æ•ˆé€‰æ‹©")
            return
            
    elif choice == "4":
        # æ˜¾ç¤ºç±»åˆ«åˆ—è¡¨
        print("\nğŸ“‹ æµ‹è¯•ç±»åˆ«è¯¦æƒ…:")
        for category, info in runner.test_cases.items():
            test_count = len(info["test_cases"])
            print(f"\nğŸ” {category}")
            print(f"   æè¿°: {info['description']}")
            print(f"   æ£€ç´¢ç­–ç•¥: {info['retrieval_strategy']}")
            print(f"   æµ‹è¯•ç”¨ä¾‹æ•°: {test_count}")
            
            for test_case in info["test_cases"]:
                print(f"   - {test_case['id']}: {test_case['query_cn']}")
        
        return
    
    else:
        print("æ— æ•ˆé€‰æ‹©")
        return
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main()) 